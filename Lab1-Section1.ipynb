{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='text-aling:center;color:Navy'>  Big Data Systems - Laboratory 1  </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics included in this lab:\n",
    " 1) Map-Reduce (20 minutes)\n",
    " 2) Semistructured DS: Spanner DB (20 minutes)\n",
    " 3) Bigtable (20 minutes)\n",
    "\n",
    "We have allocated 15 minutes for set up and/or to address any other issues. \n",
    "\n",
    "\n",
    "This is the first Lab for the Big Data Systems course - Spring 2018.<br>\n",
    "We are covering the following topics:\n",
    "<li>Map-Reduce,</li>\n",
    "<li>Spanner and</li>\n",
    "<li>Bigtable</li>\n",
    "<br>\n",
    "The lab will have an in-class section as well as a homework section. <br>\n",
    "You need to submit your in-class notebook before the end of the class. For the homework section, please refer to Canvas for the due date.\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#3665af\">Section #1: Map Reduce </span><span style=\"font-size:15px\">(Estimated time: 20 minutes) </span>\n",
    "\n",
    "<hr>\n",
    "In this section, we will practice how to use map-reduce on a hadoop enviroment in Java. \n",
    "## Pre-reqs:\n",
    "You need to have your environment set up on Google Cloud working. Please refer to the Google Cloud instructions for this setup. \n",
    "\n",
    "## Uploading your files to the cloud\n",
    "Before we can access our files from our cluster, we need to upload them to the master so we can later add the files to the HDFS. There are at least two ways to upload the files to the master Hadoop node.\n",
    "1. Creating ssh keys to the master and uploading using the SCP command. \n",
    "2. Using google buckets to store the files.\n",
    "We will use the second approach. \n",
    "\n",
    "## Creating a Bucket and uploading it. \n",
    "1. Go to the cloud console.\n",
    "2. Using the menu select Storage and then Create a bucket\n",
    "3. Name your bucket. The name should be globally unique, so choose bigdatasystem\\_{student_id}\\_bucket\n",
    "\n",
    "Once created you can upload the files to the bucket using the web interface.\n",
    "\n",
    "## Uploading files to the HDFS\n",
    "\n",
    "First we need to download the files in the bucket to the linux filesystem. To do that:\n",
    "\n",
    "- Open the SSH interface through your VM dashboard in the google cloud console. \n",
    "On the console execute:\n",
    "\n",
    "Go to your home directory and create a BDS directory\n",
    "```\n",
    "$> cd ~\n",
    "$> mkdir BDS\n",
    "$> cd ~/BDS\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Download the files. You should complete the command with your bucket name and the file name you want to download. \n",
    "```\n",
    "$> gsutil cp gs://[BUCKET_NAME]/[OBJECT_NAME] .\n",
    "```\n",
    "\n",
    "_**EXAMPLE:**_\n",
    "```\n",
    "$> gsutil cp gs://bigdatasystems_555555_bucket/hamlet.txt .\n",
    "```\n",
    "\n",
    "> Note: \\$\\> represents the linux prompt\n",
    "\n",
    "[https://cloud.google.com/storage/docs/object-basics#download]\n",
    "\n",
    "### Once we have the files on the server:\n",
    "<li>Run these commands to create the directory and to upload the input files:\n",
    "    \n",
    "```\n",
    "$> hdfs dfs -ls -al          #This will list your files\n",
    "\n",
    "$> hdfs dfs -mkdir /input     #This create the input directory on HDFS\n",
    "\n",
    "$> hdfs dfs -put {path}/{filename} {hdfs path/filename}   #This will upload the file to the DFS onther the path\n",
    "\n",
    "e.g.:\n",
    "$> hdfs dfs -put ~/BDS/hamlet.txt ~/input\n",
    "```\n",
    "Here you can find a cheat sheet of the HDFS commands [https://dzone.com/articles/hdfs-cheat-sheet]\n",
    "\n",
    "Now that we have our data in the HDFS we can program our map-reduce classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Count Example\n",
    "<hr>\n",
    "## Map/Reduce Classes\n",
    "We have three classes available:<br>\n",
    "<li>WordCount.Java              *#Main program*\n",
    "<li>WordcountMapper.java        *#Implementation of the Map*\n",
    "<li>WordCountReducer.java       *#Implementation of the Reduce*\n",
    "<br>\n",
    "Take a look at the Main program, try to identify where the files are loaded.\n",
    "<br>\n",
    "In the same fashion, inspect the Mapper and the Reduccer class to identify the emit and the collect steps on the code. \n",
    "<br> \n",
    "Now, let's run the WordCount:\n",
    "1) In your google console, go to the directory where your files are set.\n",
    "``` \n",
    "$> mkdir -p ~/BDS/mapreduce/WordCount\n",
    "$> cd ~/BDS/mapreduce/WordCount\n",
    "```\n",
    "    \n",
    "**Get the java files from your bucket to that directory.**\n",
    "We need to compile our program, \"jar\" it so we can send it to Haddop and then run it. \n",
    "\n",
    "### Compiling:\n",
    "``` \n",
    "$> pwd  #Make sure you are at ~/BDS/mapreduce/WordCount\n",
    "$> export HADOOP_CLASSPATH=$(hadoop classpath)\n",
    "$> javac -classpath ${HADOOP_CLASSPATH} -d . *.java\n",
    "```\n",
    "\n",
    "### Making the JAR\n",
    "``` \n",
    "$> pwd  #Make sure you are at ~/BDS/mapreduce/WordCount\n",
    "$> jar -cvf WordCount.jar -C . .\n",
    "```\n",
    "\n",
    "### Running hadoop Program\n",
    "``` \n",
    "$> pwd  #Make sure you are at ~/BDS/mapreduce/WordCount\n",
    "$> hadoop jar WordCount.jar WordCount /input /output\n",
    "```\n",
    "\n",
    "## Paste your output in the next cell"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "##### OUTPUT Goes HERE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using the _**hdfs dfs -get**_ bring the files in the output directory of the HDFS.\n",
    "- Using the _**head**_ command extract the first 10 rows of one of the output files. \n",
    "\n",
    "## Paste your output in the next cell"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "##### OUTPUT Goes HERE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inspect the output by using the commands cat, grep, tail, head.\n",
    "- Did you notice any issues on how the counting was done? <br>For example, look a the word \"them\". Was the output as you expected to be?\n",
    "- If not, how can you address the problem. \n",
    "\n",
    "## Write your answer in the next cell. You don't need to implement the actual response. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "##### OUTPUT Goes HERE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:RED\">Homework: </span> Department monthly sales\n",
    "<hr>\n",
    "Based on the map-reduce example above, do the proper modifications to solve the following problem:\n",
    "<br>\n",
    "We are going to be using the Walmart Store Sales Forecasting competition training dataset available [here](https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/data). Considering this dataset, you should produce a monthly summary of the sales per department. <br>\n",
    "So you output should be like:\n",
    "<pre>\n",
    "dept-year-month:$$$\n",
    "</pre>\n",
    "1. Download the training dataset and upload it to your server for processing. Be careful with the paths you use.\n",
    "2. Using the three provided java classes as a template, create three new classes: __*Walmart.java, WalmartMapper.java, WalmartReducer.java*__ with the proper code.\n",
    "3. Fill in the following and submit all your files on Canvas to the Assignment 1.\n",
    "\n",
    "Complete the following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Place your map function in the next cell:  <span style=\"font-size:12px\">(Only the function) </span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OUTPUT Goes HERE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Place your reduce function in the next cell:  <span style=\"font-size:12px\">(Only the function) </span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OUTPUT Goes HERE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Place your running output in the next cell:  <span style=\"font-size:12px\"> </span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OUTPUT Goes HERE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Place the first 10 rows of your exit in the next cell:  <span style=\"font-size:12px\">(take a look at the _wc_ linux command) </span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OUTPUT Goes HERE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 3px double navy;\" ><br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
